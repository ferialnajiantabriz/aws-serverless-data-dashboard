# **AWS Serverless Data Dashboard (Event Ingestion + Daily Analytics Pipeline)**

A fully implemented, production-style **serverless analytics pipeline** built on AWS.
This project ingests real-time events, stores them in DynamoDB, aggregates daily metrics using scheduled Lambdas, and visualizes results through a Streamlit dashboard.

The system demonstrates practical cloud engineering ability, event-driven architecture, and hands-on deployment of multiple AWS services working together seamlessly.

---

## ** High-Level Overview**

This pipeline follows a widely used pattern in real-world systems such as:

* product analytics
* marketing funnel tracking
* IoT device events
* user behavior monitoring
* app telemetry pipelines

**Flow:**

1. Clients send events (click, signup, view, etc.)
2. API Gateway receives and validates the request
3. Lambda stores the event in DynamoDB
4. EventBridge triggers a nightly aggregation
5. Another Lambda builds a daily summary
6. Summary JSON goes into S3
7. Streamlit dashboard reads DynamoDB for analytics

---

## **ğŸ—ï¸ Architecture**

![Architecture](docs/architecture.png)

---

## **âœ¨ Key Features**

### ** Real-Time Event Ingestion API**

A public HTTP endpoint handles event ingestion through API Gateway â†’ Lambda â†’ DynamoDB.

Example request:

```json
POST /event
{
  "source": "web",
  "event_type": "click",
  "meta": { "user": "demo" }
}
```

The ingestion Lambda automatically:

* generates a unique primary key (`pk`)
* assigns a timestamp (`ts`)
* stores the event in DynamoDB
* returns the saved record

---

### ** Automated Daily Aggregation**

Every night at midnight UTC, EventBridge triggers an aggregation Lambda that:

* scans DynamoDB for the last 24 hours
* groups events by type
* calculates counts
* generates a summary document
* uploads it to S3 in folder:

```
daily_summaries/YYYY-MM-DD.json
```

Example output from S3:

![Summary](docs/s3_summary_sample.png)

---

### ** Interactive Local Analytics Dashboard (Streamlit)**

A lightweight UI for exploring the ingested data:

* total event count
* bar chart of event types
* raw event table
* auto-refreshes when new data arrives

Run locally:

```bash
source .venv/bin/activate
export AWS_REGION=us-east-1
export TABLE_NAME=serverless-dashboard-dev-events
cd src/streamlit_app
streamlit run app.py
```

Dashboard example:

![Dashboard](docs/dashboard_sample.png)

---

## ** Tech Stack**

### **AWS Services**

* **Lambda (Python)** â€” compute for ingestion & aggregation
* **DynamoDB** â€” high-performance NoSQL event store
* **API Gateway (HTTP API)** â€” public ingestion endpoint
* **EventBridge** â€” nightly cron scheduler
* **S3** â€” archive of daily rollups
* **CloudWatch** â€” logging & alarms
* **CloudFormation** â€” infrastructure-as-code

### **Local Tools**

* Streamlit (dashboard)
* Python 3.12
* boto3 (AWS SDK)
* requests

---

## ** Deployment**

Deploy the full stack (Lambdas, API Gateway, DynamoDB, EventBridge, S3, IAM roles):

```bash
./scripts/deploy.sh us-east-1
```

Remove everything:

```bash
./scripts/teardown.sh us-east-1
```

Validate the CloudFormation template manually:

```bash
aws cloudformation validate-template \
  --template-body file://template.yaml \
  --region us-east-1
```

---

## ** Repository Structure**

```
aws-serverless-data-dashboard/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest/           # Lambda: event ingestion
â”‚   â”œâ”€â”€ aggregate/        # Lambda: daily aggregation
â”‚   â””â”€â”€ streamlit_app/    # Local dashboard
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ deploy.sh         # Automated stack deployment
â”‚   â””â”€â”€ teardown.sh       # Automated resource cleanup
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.png
â”‚   â”œâ”€â”€ dashboard_sample.png
â”‚   â”œâ”€â”€ dynamodb_sample.png
â”‚   â””â”€â”€ s3_summary_sample.png
â”‚
â”œâ”€â”€ template.yaml         # Full CloudFormation stack
â””â”€â”€ README.md
